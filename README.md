# Viable Data tech test
## Setup for running applications and tests
Navigate to the `viabledata` root directory (the same one this file is in)
Activate virtual environment and install dependencies
```
source ../venv/bin/activate
pip3 install -r requirements.txt
```

### Run migrations and load the dummy data
```
python3 manage.py migrate
python3 manage.py loaddata dummydata
```
### Start the application
```
python3 manage.py runserver
```

Navigate to `http://localhost:8000/api/` to view the API schema

### Run tests if you want
```
export DJANGO_SETTINGS_MODULE=viabledata.settings
pytest
```

## Notes
* You can access [Django admin](http://localhost:8000/admin/) if you need to for any reason. You will need to create a user so run `python3 manage.py createsuperuser` and follow the instructions.
* I used DRF Spectacular which automatically generates the Open API spec and uses it to serve a working API client on a specific URL. This makes it easy to develop as you can see your changes quickly and can even play with the API. It also importantly documents the API and since it's autogenerated from the code, it's never out-of-date.
* The search functionality is documented in the `GET /companies/` endpoint's documentation in the Swagger UI. Navigate to the API docs `http://localhost:8000/api/` and try it out for yourself. The search may not work for all cases envisioned by the author of this tech test, but that could be ironed out if this application was underwent proper development.
* I used `black`, `isort` and `autoflake` for linting. In a real project these would be configured using a pyproject.toml file and be a part of pre-commit hooks. The result would be that all committed code would automatically be linted in a consistent way, avoiding unimportant discussions over code formatting.
* The deserialized representation of a company is quite big and extensive. It could be simpler â€“ for example, instead of the `tax_infos` value being an array of tax info objects which contain the id, company ID, type (NINO or SSN), value, it could simply be an array of the values. This would depend on how the API is intended to be used by the client. Also, there is no reason why we couldn't have multiple serializers for the company object which would each be used depending on the use case. This would help ensure the application is performant and doesn't waste database queries or have an unnecessarily large payload size. 

## What I would change with more time
* Add a users app and a custom users model depending on what the project requirements are.
* The application only supports updating fields on the company model (such as email address, name etc.), not any of the related models like bank accounts, trades, tax info etc. Adding support for that would involve creating DRF viewsets for each of the models so in the interest of saving time, I opted to not do this.
* Add validation to the fields. Examples:
  * Sort code should be 6 digits
  * National Insurance number should follow the rules (regex-based validation would be sufficient)
* Add a static type checker such as `mypy` to enforce type safety. I didn't add it because getting it to behave with Django takes a bit of time and I didn't want to spend extra time doing that. 
* I tried to use Django's `select_related` in the company view so Django would join the relevant tables when fetching the objects from the database. This would prevent the need for multiple queries which would slow down the API. However, when testing, the number of database queries was not reduced. With more time I would investigate this and try to reduce the number of queries.
* Write tests for the search functionality. These tests could look something like this (just pseudocode):
```python
def test_filter_on_qualification_level():
    # set up test data
    company1 = Company(trade=Trade('carpentry', level='basic'))
    company2 = Company(trade=Trade('birdwatching', level='advanced'))
    
    # call API
    response = client.get("/companies/", params={"level": "advanced"})
    
    # assertions
    assert company1 not in response  # not in results as it only has a basic trade
    assert company2 in response
```
Many of these types of tests could be written to test the various combinations and also to ensure that the searching works exactly as intended. Any bugs found could be turned into failing test cases, the code fixed, tests then pass (TDD).
* The search functionality is performant so far on a small amount of test data. This should be a decent for an initial version of the application. Over time, we would gain insights into the usage patterns and how the search performs with a larger load. After this, optimisation of search may be required at which point we can dig into optimising the database queries or possibly even using a different search backend such as OpenSearch.